# -*- coding:utf-8 -*-
# @FileName  :realtime_sensevoice.py
# @Time      :2024/8/31 16:45
# @Author    :modified for real-time processing

import argparse
import logging
import os
import queue
import threading
import time
import numpy as np
import sounddevice as sd
import soundfile as sf
import noisereduce
from collections import deque
from typing import Optional, List
from datetime import datetime

# ê¸°ì¡´ ëª¨ë“ˆë“¤ import (íŒŒì¼ì—ì„œ ê°€ì ¸ì˜¨ í´ë˜ìŠ¤ë“¤)
from sensevoice_rknn import WavFrontend, FSMNVad, SenseVoiceInferenceSession

# ë¡œê¹… ì„¤ì •
formatter = "%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(message)s"
logging.basicConfig(format=formatter, level=logging.INFO)

# ì–¸ì–´ ì„¤ì •
languages = {"auto": 0, "zh": 3, "en": 4, "yue": 7, "ja": 11, "ko": 12, "nospeech": 13}

class SpecialTokenParser:
    """SenseVoiceì˜ special tokenì„ íŒŒì‹±í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.language_tokens = {
            "<|zh|>": "zh",
            "<|en|>": "en", 
            "<|yue|>": "yue",
            "<|ja|>": "ja",
            "<|ko|>": "ko"
        }
        
        self.emotion_tokens = {
            "<|HAPPY|>": "HAPPY",
            "<|SAD|>": "SAD",
            "<|ANGRY|>": "ANGRY", 
            "<|NEUTRAL|>": "NEUTRAL",
            "<|FEARFUL|>": "FEARFUL",
            "<|DISGUSTED|>": "DISGUSTED",
            "<|SURPRISED|>": "SURPRISED",
            "<|EMO_UNKNOWN|>": "EMO_UNKNOWN"
        }
        
        self.event_tokens = {
            "<|BGM|>": "BGM",
            "<|Speech|>": "Speech",
            "<|Applause|>": "Applause",
            "<|Laughter|>": "Laughter",
            "<|Cry|>": "Cry",
            "<|Sneeze|>": "Sneeze",
            "<|Breath|>": "Breath",
            "<|Cough|>": "Cough"
        }
        
        self.itn_tokens = {
            "<|withitn|>": "with_itn",
            "<|woitn|>": "without_itn"
        }
        
        # ëª¨ë“  special token íŒ¨í„´
        self.all_tokens = {}
        self.all_tokens.update(self.language_tokens)
        self.all_tokens.update(self.emotion_tokens)
        self.all_tokens.update(self.event_tokens)
        self.all_tokens.update(self.itn_tokens)
    
    def parse_result(self, text: str) -> tuple:
        """
        í…ìŠ¤íŠ¸ì—ì„œ special tokenì„ íŒŒì‹±í•©ë‹ˆë‹¤.
        
        Returns:
            tuple: (clean_text, token_info_dict)
        """
        if not text or not text.strip():
            return "", {}
        
        original_text = text
        token_info = {
            "lang": None,
            "emotion": None, 
            "event": None,
            "itn": None,
            "transcript": ""
        }
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ í† í° ì¶”ì¶œ
        for token, value in self.language_tokens.items():
            if token in text:
                token_info["lang"] = value
                text = text.replace(token, "")
        
        for token, value in self.emotion_tokens.items():
            if token in text:
                token_info["emotion"] = value
                text = text.replace(token, "")
        
        for token, value in self.event_tokens.items():
            if token in text:
                token_info["event"] = value
                text = text.replace(token, "")
        
        for token, value in self.itn_tokens.items():
            if token in text:
                token_info["itn"] = value
                text = text.replace(token, "")
        
        # ë‚¨ì€ í…ìŠ¤íŠ¸ê°€ ì‹¤ì œ ì „ì‚¬ ê²°ê³¼
        clean_text = text.strip()
        token_info["transcript"] = clean_text
        
        return clean_text, token_info
    
    def extract_clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ special tokenì„ ì œê±°í•˜ê³  ê¹¨ë—í•œ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜"""
        clean_text, _ = self.parse_result(text)
        return clean_text

class RealTimeSenseVoice:
    def __init__(
        self, 
        model_path: str, 
        device_id: int = -1, 
        num_threads: int = 4,
        language: str = "auto",
        use_itn: bool = False,
        sample_rate: int = 16000,
        chunk_duration: float = 0.2,  # 0.2ì´ˆì”© ì²˜ë¦¬ (VAD ì•ˆì •ì„± í–¥ìƒ)
        silence_threshold: float = 0.3,  # 0.3ì´ˆ ì¹¨ë¬µ ì„ê³„ê°’
        max_sentence_duration: float = 10.0,  # ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ 10ì´ˆ
        save_recordings: bool = True,  # WAV íŒŒì¼ ì €ì¥ ì—¬ë¶€
        recordings_dir: str = "./recordings"  # ë…¹ìŒ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
    ):
        self.sample_rate = sample_rate
        self.chunk_duration = chunk_duration
        self.chunk_size = int(sample_rate * chunk_duration)
        self.language = languages[language]
        self.use_itn = use_itn
        self.silence_threshold = silence_threshold
        self.max_sentence_duration = max_sentence_duration
        self.save_recordings = save_recordings
        self.recordings_dir = recordings_dir

        # ë…¹ìŒ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        if self.save_recordings:
            os.makedirs(self.recordings_dir, exist_ok=True)
        
        # ë¬¸ì¥ ë‹¨ìœ„ ë²„í¼ ê´€ë¦¬
        self.sentence_buffer = []  # í˜„ì¬ ë¬¸ì¥ì„ ë‹´ëŠ” ë²„í¼
        self.silence_duration = 0.0  # í˜„ì¬ ì¹¨ë¬µ ì§€ì† ì‹œê°„
        self.last_inference_length = 0  # ë§ˆì§€ë§‰ ì¶”ë¡  ì‹œì ì˜ ë²„í¼ ê¸¸ì´
        self.inference_interval = 1.0  # 1.0ì´ˆë§ˆë‹¤ ì¶”ë¡ 
        self.in_sentence = False  # í˜„ì¬ ë¬¸ì¥ ì¤‘ì¸ì§€ ì—¬ë¶€

        # ì˜¤ë””ì˜¤ ì…ë ¥ í
        self.audio_queue = queue.Queue()
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_models(model_path, device_id, num_threads)

        # Token Parser ì´ˆê¸°í™”
        self.token_parser = SpecialTokenParser()

        # ìŠ¤ë ˆë”© ì œì–´
        self.is_running = False
        self.processing_thread = None

        self.prev_chunks = deque(maxlen=1)
        self.last_result = None
        
    def _initialize_models(self, model_path: str, device_id: int, num_threads: int):
        """ëª¨ë¸ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        try:
            # Frontend ì´ˆê¸°í™”
            self.frontend = WavFrontend(os.path.join(model_path, "am.mvn"))
            
            # SenseVoice ëª¨ë¸ ì´ˆê¸°í™”
            self.model = SenseVoiceInferenceSession(
                os.path.join(model_path, "embedding.npy"),
                os.path.join(model_path, "sense-voice-encoder.rknn"),
                os.path.join(model_path, "chn_jpn_yue_eng_ko_spectok.bpe.model"),
                device_id,
                num_threads,
            )
            
            # VAD ëª¨ë¸ ì´ˆê¸°í™”
            self.vad = FSMNVad(model_path)
            
            logging.info("All models initialized successfully")
            
        except Exception as e:
            logging.error(f"Failed to initialize models: {e}")
            raise
    
    def audio_callback(self, indata, frames, time, status):
        """ë§ˆì´í¬ ì…ë ¥ ì½œë°± í•¨ìˆ˜"""
        if status:
            logging.warning(f"Audio callback status: {status}")
        
        # ëª¨ë…¸ ì±„ë„ë¡œ ë³€í™˜
        if len(indata.shape) > 1 and indata.shape[1] > 1:
            audio_data = indata[:, 0]
        else:
            audio_data = indata.flatten()
        
        # íì— ì˜¤ë””ì˜¤ ë°ì´í„° ì¶”ê°€
        self.audio_queue.put(audio_data.copy())

    def save_audio_file(self, audio_data: np.ndarray, transcription: str = "") -> str:
        """ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ WAV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        if not self.save_recordings:
            return None
        
        try:
            # ì˜¤ë””ì˜¤ ë°ì´í„° ê²€ì¦
            if len(audio_data) == 0:
                logging.warning("Empty audio data, skipping save")
                return None
            
            # í˜„ì¬ ì‹œê°ì„ íŒŒì¼ëª…ì— í¬í•¨ (ë‚ ì§œ_ì‹œê°_ë°€ë¦¬ì´ˆ)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # ë°€ë¦¬ì´ˆê¹Œì§€
            filename = f"{timestamp}.wav"
            
            filepath = os.path.join(self.recordings_dir, filename)
            
            # ì˜¤ë””ì˜¤ ë°ì´í„° ì •ê·œí™” (-1.0 ~ 1.0 ë²”ìœ„ë¡œ)
            if audio_data.dtype != np.float32:
                audio_data = audio_data.astype(np.float32)
            
            # ë³¼ë¥¨ì´ ë„ˆë¬´ í° ê²½ìš° ì •ê·œí™”
            max_val = np.abs(audio_data).max()
            if max_val > 1.0:
                audio_data = audio_data / max_val
            
            # WAV íŒŒì¼ë¡œ ì €ì¥
            sf.write(filepath, audio_data, self.sample_rate, subtype='PCM_16')
            
            logging.info(f"Audio saved: {filepath} (duration: {len(audio_data)/self.sample_rate:.2f}s)")
            return filepath
            
        except ImportError:
            logging.error("soundfile library not installed. Please install: pip install soundfile")
            return None
        except Exception as e:
            logging.error(f"Failed to save audio file: {e}")
            return None
    
    def has_speech_in_chunk(self, audio_chunk: np.ndarray) -> bool:
        """ì²­í¬ì— ìŒì„±ì´ ìˆëŠ”ì§€ VADë¡œ í™•ì¸"""
        try:
            # VAD ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” ìµœì†Œ ê¸¸ì´ í™•ì¸
            min_length = int(self.sample_rate * 0.2)
            if len(audio_chunk) < min_length:
                # íŒ¨ë”©ìœ¼ë¡œ ìµœì†Œ ê¸¸ì´ ë§ì¶”ê¸°
                padded_chunk = np.pad(audio_chunk, (0, min_length - len(audio_chunk)), 'constant')
            else:
                padded_chunk = audio_chunk
            
            # VAD ìˆ˜í–‰
            segments = self.vad.segments_offline(padded_chunk)
            
            # ê²°ê³¼ ê²€ì¦
            if segments is None:
                return False
            
            return len(segments) > 0
            
        except IndexError as e:
            # logging.warning(f"VAD IndexError - likely empty audio: {e}")
            return False
        except Exception as e:
            logging.error(f"Error in VAD detection: {e}")
            return False
    
    def perform_inference(self, audio_data: np.ndarray) -> Optional[str]:
        """ì˜¤ë””ì˜¤ ë°ì´í„°ì— ëŒ€í•´ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            if len(audio_data) < self.sample_rate * 0.3:  # 0.3ì´ˆ ë¯¸ë§Œì€ ê±´ë„ˆë›°ê¸°
                return None

            if np.any(np.isnan(audio_data)) or np.any(np.isinf(audio_data)):
                audio_data = np.nan_to_num(audio_data, nan=0.0, posinf=0.0, neginf=0.0)

            # denoise
            audio_data = noisereduce.reduce_noise(y=audio_data, sr=self.sample_rate)
            
            # íŠ¹ì§• ì¶”ì¶œ
            audio_feats = self.frontend.get_features(audio_data)

            # ASR ìˆ˜í–‰
            asr_result = self.model(
                audio_feats[None, ...],
                language=self.language,
                use_itn=self.use_itn,
            )

            if asr_result and asr_result.strip():
                self.last_result = asr_result.strip()
                return self.last_result

            return None
            
        except Exception as e:
            logging.error(f"Error in inference: {e}")
            return self.last_result
    
    def should_perform_inference(self) -> bool:
        """ì¶”ë¡ ì„ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨"""
        current_length = len(self.sentence_buffer) / self.sample_rate
        return current_length >= self.last_inference_length + self.inference_interval

    def should_finalize_sentence(self) -> bool:
        """ë¬¸ì¥ì„ ì™„ë£Œí•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨"""
        buffer_duration = len(self.sentence_buffer) / self.sample_rate
        return (self.silence_duration >= self.silence_threshold or 
                buffer_duration >= self.max_sentence_duration)
    
    def finalize_sentence(self):
        """ë¬¸ì¥ì„ ìµœì¢…ì ìœ¼ë¡œ ì™„ë£Œí•©ë‹ˆë‹¤."""
        # ìµœì¢… ì¶”ë¡  ìˆ˜í–‰
        final_buffer = np.array(self.sentence_buffer)
        final_result = self.perform_inference(final_buffer)

        if final_result is None:
            final_result = self.last_result

        # WAV íŒŒì¼ ì €ì¥
        saved_file = self.save_audio_file(final_buffer)
        if saved_file:
            print(f"ğŸ’¾ ì €ì¥ë¨: {os.path.basename(saved_file)}")
                    
        if final_result:
            # Special Token íŒŒì‹±
            clean_text, token_info = self.token_parser.parse_result(final_result)

            if len(clean_text.strip()) == 0:
                print(f"Sentence is too short. ({clean_text})")
            else:
                current_time = time.strftime('%H:%M:%S')
                buffer_duration = len(self.sentence_buffer) / self.sample_rate
                print(f"\râœ… [{current_time}] ì™„ë£Œ ({buffer_duration:.1f}s): {clean_text}")
                print(f"   Token Info: {token_info}")
                            
                # ê²°ê³¼ ì½œë°± í˜¸ì¶œ
                if hasattr(self, 'result_callback'):
                    self.result_callback(clean_text, token_info)

        else:
            print(f"Final result is empty.")

        print("End of sentence")
        # ë²„í¼ ì´ˆê¸°í™”
        self.sentence_buffer = []
        self.silence_duration = 0.0
        self.last_inference_length = 0
                    
        # VAD ìƒíƒœ ë¦¬ì…‹
        self.frontend.reset_status()
        self.vad.vad.all_reset_detection()
    
    def process_audio_stream(self):
        """ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„ ì§€ì†ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        logging.info("Starting audio processing thread")
        
        current_chunk_buffer = []
        
        while self.is_running:
            try:
                # íì—ì„œ ì˜¤ë””ì˜¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                audio_chunk = self.audio_queue.get(timeout=1.0)
                current_chunk_buffer.extend(audio_chunk)
                
                # ì§€ì •ëœ ì²­í¬ í¬ê¸°ê°€ ë  ë•Œê¹Œì§€ ëŒ€ê¸°
                if len(current_chunk_buffer) < self.chunk_size:
                    continue
                
                # ì²˜ë¦¬í•  ì²­í¬ ì¶”ì¶œ
                process_chunk = np.array(current_chunk_buffer[:self.chunk_size])
                current_chunk_buffer = current_chunk_buffer[self.chunk_size:]
                
                # VADë¡œ ìŒì„± êµ¬ê°„ í™•ì¸
                has_speech = self.has_speech_in_chunk(process_chunk)

                if has_speech:
                    if not self.in_sentence:
                        self.in_sentence = True
                    self.silence_duration = 0.0  # ì¹¨ë¬µ ì¹´ìš´í„° ë¦¬ì…‹
                else:
                    self.silence_duration += self.chunk_duration
                
                if self.in_sentence:
                    # ìŒì„±ì´ ìˆìœ¼ë©´ ë¬¸ì¥ ë²„í¼ì— ì²­í¬ ì „ì²´ë¥¼ ì¶”ê°€
                    if len(self.sentence_buffer) == 0 and len(self.prev_chunks) > 0:
                        # ì´ì „ ì²­í¬ë“¤ë„ ë¬¸ì¥ ë²„í¼ì— ì¶”ê°€ (ìŒì„± ì‹œì‘ ë¶€ë¶„ ë³´ì¡´)
                        for prev_chunk in self.prev_chunks:
                            self.sentence_buffer.extend(prev_chunk)
                    self.sentence_buffer.extend(process_chunk)
                    
                    # ì£¼ê¸°ì  ì¶”ë¡  ìˆ˜í–‰
                    if self.should_perform_inference():
                        buffer_array = np.array(self.sentence_buffer)
                        partial_result = self.perform_inference(buffer_array)
                        
                        if partial_result:
                            clean_text = self.token_parser.extract_clean_text(partial_result)
                            current_time = time.strftime('%H:%M:%S')
                            buffer_duration = len(self.sentence_buffer) / self.sample_rate
                            print(f"\rğŸ¤ [{current_time}] ({buffer_duration:.1f}s) {clean_text}", end='', flush=True)

                        # ë‹¤ìŒ ì¶”ë¡  ì‹œì  ì—…ë°ì´íŠ¸
                        self.last_inference_length = len(self.sentence_buffer) / self.sample_rate

                    # ë¬¸ì¥ ì™„ë£Œ ì¡°ê±´ í™•ì¸
                    if self.sentence_buffer and self.should_finalize_sentence():
                        self.finalize_sentence()
                        self.in_sentence = False

                self.prev_chunks.append(process_chunk)

            except queue.Empty:
                continue
            except Exception as e:
                logging.error(f"Error in audio processing: {e}")
                continue
    
    def start_recording(self):
        """ì‹¤ì‹œê°„ ë…¹ìŒ ë° ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
        if self.is_running:
            logging.warning("Recording is already running")
            return
        
        self.is_running = True
        
        # ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘
        self.processing_thread = threading.Thread(target=self.process_audio_stream)
        self.processing_thread.daemon = True
        self.processing_thread.start()
        
        # ì‹œì‘ ë©”ì‹œì§€
        print("=" * 60)
        print("ğŸ¤ ì‹¤ì‹œê°„ ìŒì„±ì¸ì‹ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤")
        print("ğŸ—£ï¸  ë§ˆì´í¬ì— ëŒ€ê³  ë§ì”€í•´ ì£¼ì„¸ìš”")
        print(f"âš™ï¸  ì„¤ì •: ì¹¨ë¬µ ì„ê³„ê°’ {self.silence_threshold}ì´ˆ, ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ {self.max_sentence_duration}ì´ˆ")
        print("â¹ï¸  ì¤‘ì§€í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”")
        print("=" * 60)
        
        # ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì‹œì‘
        try:
            with sd.InputStream(
                callback=self.audio_callback,
                channels=1,
                samplerate=self.sample_rate,
                blocksize=int(self.sample_rate * self.chunk_duration),
                dtype=np.float32
            ):
                # ë©”ì¸ ë£¨í”„
                while self.is_running:
                    time.sleep(0.01)
                    
        except KeyboardInterrupt:
            logging.info("Recording stopped by user")
        except Exception as e:
            logging.error(f"Error in audio recording: {e}")
        finally:
            self.stop_recording()
    
    def stop_recording(self):
        """ë…¹ìŒì„ ì¤‘ì§€í•©ë‹ˆë‹¤."""
        self.is_running = False
        
        # ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ìˆë‹¤ë©´ ì²˜ë¦¬
        if self.sentence_buffer:
            self.finalize_sentence()

        if self.processing_thread and self.processing_thread.is_alive():
            self.processing_thread.join(timeout=2.0)
        
        # ë²„í¼ í´ë¦¬ì–´
        self.sentence_buffer = []
        while not self.audio_queue.empty():
            try:
                self.audio_queue.get_nowait()
            except queue.Empty:
                break
        
        print("\nğŸ›‘ ë…¹ìŒì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        logging.info("Recording stopped and resources cleaned up")
    
    def set_result_callback(self, callback):
        """ê²°ê³¼ ì½œë°± í•¨ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
        self.result_callback = callback


def main():
    arg_parser = argparse.ArgumentParser(description="Real-time SenseVoice Speech-to-Text")
    arg_parser.add_argument(
        "-dp",
        "--download_path",
        default=os.path.dirname(__file__),
        type=str,
        help="dir path of resource downloaded",
    )
    arg_parser.add_argument("-d", "--device", default=-1, type=int, help="Device ID")
    arg_parser.add_argument(
        "-n", "--num_threads", default=4, type=int, help="Number of threads"
    )
    arg_parser.add_argument(
        "-l",
        "--language",
        choices=languages.keys(),
        default="ko",
        type=str,
        help="Language",
    )
    arg_parser.add_argument("--use_itn", action="store_true", help="Use ITN")
    arg_parser.add_argument(
        "--chunk_duration", 
        # default=0.3,
        default=0.2, 
        type=float, 
        help="Audio chunk duration in seconds"
    )
    arg_parser.add_argument(
        "--silence_threshold",
        default=0.4,
        type=float,
        help="Silence threshold in seconds to finalize sentence"
    )
    arg_parser.add_argument(
        "--max_sentence_duration",
        default=8.0,
        type=float,
        help="Maximum sentence duration in seconds"
    )
    arg_parser.add_argument(
        "--save_recordings",
        default=False,
        action="store_true",
        help="Save audio recordings as WAV files"
    )
    arg_parser.add_argument(
        "--recordings_dir",
        default="./recordings",
        type=str,
        help="Directory to save audio recordings"
    )
    
    args = arg_parser.parse_args()
    
    try:
        # ì‹¤ì‹œê°„ ìŒì„±ì¸ì‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        stt_system = RealTimeSenseVoice(
            model_path=args.download_path,
            device_id=args.device,
            num_threads=args.num_threads,
            language=args.language,
            use_itn=args.use_itn,
            chunk_duration=args.chunk_duration,
            silence_threshold=args.silence_threshold,
            max_sentence_duration=args.max_sentence_duration,
            save_recordings=args.save_recordings,
            recordings_dir=args.recordings_dir
        )
        
        # ê²°ê³¼ ì²˜ë¦¬ ì½œë°± (ì„ íƒì‚¬í•­)
        def on_result(clean_text, token_info, audio_file=""):
            # ì—¬ê¸°ì„œ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
            with open("transcription_log.txt", "a", encoding="utf-8") as f:
                timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
                f.write(f"[{timestamp}] {clean_text}\n")
                f.write(f"    Token Info: {token_info}\n")
                if audio_file:
                    f.write(f"    Audio: {audio_file}\n")
        
        stt_system.set_result_callback(on_result)
        
        # ì‹¤ì‹œê°„ ë…¹ìŒ ì‹œì‘
        stt_system.start_recording()
        
    except Exception as e:
        logging.error(f"Failed to start real-time STT: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())